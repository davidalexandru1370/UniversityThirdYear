0. C. Dennis Ritchie

   Business men - {Bill Gates, Steve Jobs, Jeff Bezos}
   Computer scientist - {Dennis Ritchie}

1. Ada Lovelace

2. The transition from the 1970s to the 1980s in the computing world marked a significant paradigm shift across multiple dimensions, including programming languages, operating systems, cultural attitudes, and software distribution models. This shift was characterized by a movement from the nascent, experimental, and idealistic beginnings of the tech industry towards a more structured, commercial, and competitive environment.

Programming Languages: C vs. C++
C Language (1970s)

Procedural Programming: The C language, developed by Dennis Ritchie at Bell Labs in the early 1970s, epitomizes procedural programming. Procedural programming emphasizes a linear top-down approach, where the program is divided into procedures or functions. Each function performs a specific task, and data is passed between functions.
System Programming: C became the lingua franca for system programming, notably being used to write the UNIX operating system. Its efficiency, control over system resources, and simplicity made it a powerful tool for developing low-level system software.
C++ Language (1980s)

Object-Oriented Programming (OOP): C++, developed by Bjarne Stroustrup in the early 1980s, extended C by incorporating object-oriented principles. OOP introduces concepts such as classes, inheritance, polymorphism, and encapsulation, allowing for more modular, reusable, and maintainable code.
Commercial Software Development: As software complexity grew, the need for better organization and modularity became paramount. C++ provided these features, aligning well with the increasing scale and complexity of commercial software projects.
Operating Systems: UNIX vs. Proprietary OSs
UNIX (1970s)

Open and Collaborative: UNIX, created at Bell Labs by Ken Thompson and Dennis Ritchie, was built with portability, multi-user capabilities, and a philosophy of simplicity and reusability. It was widely adopted in academic and research institutions.
Source Code Availability: The source code for UNIX was initially distributed to universities and research institutions, fostering a collaborative environment where improvements and adaptations were shared openly.
Proprietary OSs (1980s)

MS-DOS: Microsoft's MS-DOS, launched in 1981, became the dominant operating system for IBM PCs. It was a closed-source, proprietary system that offered simplicity and broad compatibility with emerging PC hardware.
MacOS: Apple’s MacOS, introduced with the Macintosh in 1984, featured a graphical user interface and was also proprietary. It emphasized ease of use and integration with Apple’s hardware.
Commercial Focus: These proprietary systems were developed with a strong commercial focus, aiming to create competitive advantages and market dominance.
Cultural Shift: Hippie Culture vs. Entrepreneurial Spirit
Hippie Culture (1970s)

Idealism and Collaboration: Influential figures like Dennis Ritchie and Richard Stallman embodied a countercultural ethos that valued openness, sharing, and the free exchange of ideas. This period saw the birth of foundational technologies and ideas driven by intellectual curiosity and a communal spirit.
Foundations of Free Software: Richard Stallman’s work at MIT and his founding of the Free Software Foundation in 1985 laid the groundwork for the open-source movement, emphasizing software freedom and community-driven development.
Entrepreneurial Spirit (1980s)

Commercial Ambition: The 1980s saw the rise of tech entrepreneurs like Bill Gates (Microsoft) and Steve Jobs (Apple). They were driven by the commercial potential of personal computing, focusing on building successful, profit-driven businesses.
Market Competition: This era was marked by intense competition, strategic partnerships, and aggressive marketing. The focus shifted from collaborative development to gaining competitive advantage and market share.
Software Distribution: Open Source vs. Proprietary Code
Open Source (1970s-early 1980s)

Community Development: Early software development was characterized by a collaborative approach. Code was often shared freely among researchers and developers, fostering innovation and improvement through community input.
GNU Project: Richard Stallman’s GNU Project (1983) aimed to create a completely free Unix-like operating system, advocating for the principles of free software where users could run, modify, and share software without restriction.
Proprietary Code (1980s)

Commercialization of Software: The 1980s saw a shift towards proprietary software models, where companies like Microsoft and Apple developed closed-source software that they sold under restrictive licenses.
Intellectual Property: Proprietary software emphasized the protection of intellectual property, with source code kept secret to maintain competitive advantages and maximize revenue.
Conclusion
The paradigm shift from the 1970s to the 1980s in computing was profound, characterized by a movement from procedural to object-oriented programming, from open, collaborative UNIX systems to closed, proprietary operating systems, and from a countercultural, idealistic ethos to a commercially driven, entrepreneurial spirit. This transformation laid the foundation for the modern tech industry, balancing the ideals of open source and community with the realities of commercial software development and market competition.

3.

4. The advent of Graphical Processing Units (GPUs) has marked a significant milestone in the evolution of computer games, significantly enhancing their visual and interactive capabilities. Let's compare a legendary game from before the advent of GPUs with one created after this technological landmark and discuss how this hardware advancement has influenced current industry trends such as Deep Learning and Cryptocurrency.

Legendary Games: Pre-GPU and Post-GPU
Pre-GPU Game: "Pac-Man" (1980)

Overview: Released by Namco in 1980, "Pac-Man" is an iconic arcade game that became a cultural phenomenon. It features a simple 2D maze where players navigate Pac-Man to eat pellets while avoiding ghosts.
Technology: "Pac-Man" was developed for arcade machines with basic microprocessors and no specialized graphical hardware. The graphics were simple, using sprites for characters and static backgrounds.
Gameplay: The gameplay focused on strategy and quick reflexes, with no need for complex graphics. The charm and addictiveness of "Pac-Man" lay in its straightforward yet challenging design.
Post-GPU Game: "The Witcher 3: Wild Hunt" (2015)

Overview: Developed by CD Projekt Red, "The Witcher 3: Wild Hunt" is a critically acclaimed action role-playing game known for its expansive open world, deep narrative, and stunning visuals.
Technology: "The Witcher 3" leverages modern GPUs to render highly detailed environments, complex character models, and realistic lighting effects. The game uses advanced graphical techniques such as tessellation, ambient occlusion, and dynamic weather systems.
Gameplay: The enhanced graphical capabilities contribute to an immersive experience, allowing for intricate world-building and rich storytelling. The visuals play a crucial role in drawing players into the game's universe.
Impact of GPUs on Industry Trend-Setters
Deep Learning

GPUs and Parallel Processing: GPUs excel at parallel processing, which is essential for training deep neural networks. The ability to perform many calculations simultaneously significantly accelerates the training process of models used in tasks like image and speech recognition.
Advancements in AI: The rise of GPUs has been pivotal in the development of advanced AI and machine learning algorithms. Companies like NVIDIA have tailored their hardware (e.g., CUDA cores) to optimize deep learning operations, making GPUs a cornerstone of AI research and development.
Applications: From autonomous vehicles to natural language processing, the improvements in deep learning driven by GPUs have broad implications. This hardware has enabled breakthroughs in areas that require extensive data processing and real-time computation.
Cryptocurrency

Mining Efficiency: In the cryptocurrency domain, GPUs are widely used for mining due to their superior ability to handle the repetitive, parallel tasks required for solving cryptographic puzzles. This efficiency makes them ideal for mining cryptocurrencies like Bitcoin and Ethereum.
Evolution of Mining Hardware: The shift from CPUs to GPUs has dramatically increased the efficiency and profitability of mining operations. This has led to the development of specialized mining rigs and a booming market for high-performance GPUs.
Blockchain Applications: Beyond mining, GPUs support various applications in blockchain technology, including the execution of smart contracts and the development of decentralized applications (DApps). The computational power of GPUs ensures that these processes are carried out efficiently and securely.
Conclusion
The evolution from "Pac-Man" to "The Witcher 3: Wild Hunt" illustrates the transformative impact of GPUs on gaming, enhancing graphical fidelity and immersive experiences. Beyond gaming, GPUs have revolutionized fields like deep learning and cryptocurrency, driving advancements through their unparalleled processing power. This hardware innovation continues to shape the future of technology, enabling new applications and pushing the boundaries of what is possible in computational tasks.

5. Let's separate and sort the software and hardware entities chronologically, then connect each software concept to its hardware counterpart.

Hardware Entities
Computing Machine
Harddisk
Internet
Software Entities
Operating System
Filesystem
The World Wide Web
Web Server
E-commerce Site
Chronological Order and Connections
Computing Machine

Operating System: The operating system is the fundamental software that manages the hardware of a computing machine, allowing other software to run.
Harddisk

Filesystem: The filesystem is software that manages how data is stored and retrieved on the harddisk, organizing files and directories.
Internet

The World Wide Web: The World Wide Web is a system of interlinked hypertext documents and multimedia content accessed via the Internet.
Web Server: A web server is software that uses the Internet to deliver web pages to users' browsers by responding to their HTTP requests.
E-commerce Site: An e-commerce site is a specialized web application hosted on a web server, enabling commercial transactions over the World Wide Web.
Explanation
Computing Machine and Operating System: Early computers required basic software to manage hardware resources, leading to the development of operating systems like UNIX and MS-DOS.
Harddisk and Filesystem: As data storage needs grew, filesystems like FAT (File Allocation Table) and later NTFS (New Technology File System) were developed to efficiently manage data on harddisks.
Internet and The World Wide Web: The Internet, initially developed as a network for researchers, became the foundation for the World Wide Web in the early 1990s. This allowed for the creation and linking of web pages, forming the global web.
Internet and Web Server: With the advent of the World Wide Web, web servers like Apache and Nginx were developed to serve web pages over the Internet.
Internet and E-commerce Site: The commercialization of the web led to the creation of e-commerce sites like Amazon and eBay, which rely on web servers and the Internet to facilitate online shopping.
By separating and organizing these entities, we can see the progression from fundamental hardware to complex software systems and how each software concept builds upon the hardware and earlier software innovations.

6. Here's a chronological timeline of the events, illustrating causal links where applicable:

Timeline of Events
Babylonian clay tablets holding interesting mathematical triples of numbers (~1800 BCE)
Pitagora's cult (~500 BCE)
Edison’s lightbulb (1879)
ENIAC (1945)
Frank Rosenblatt's Perceptron (1958)
TCP/IP is developed in order to build the ARPANET (1983)
Sir Tim Berners-Lee introduces the HyperText Transfer Protocol (1989)
The American "moon landing" (1969)
The second AI winter (1980s - 1990s)
Deep neural networks (2000s - present)
Causal Links and Explanation
Babylonian clay tablets holding interesting mathematical triples of numbers (~1800 BCE)

Pitagora's cult (~500 BCE): The Babylonian tablets included mathematical discoveries that influenced later mathematicians, including Pythagoras and his followers, who built upon these early ideas.
Edison’s lightbulb (1879)

ENIAC (1945): Edison’s advancements in electrical engineering and the widespread adoption of electricity set the stage for the development of electronic devices like the ENIAC, the first general-purpose electronic digital computer.
ENIAC (1945)

Frank Rosenblatt's Perceptron (1958): The development of computers like the ENIAC enabled the exploration of early artificial intelligence concepts, including Rosenblatt’s Perceptron, an early neural network model.
Frank Rosenblatt's Perceptron (1958)

The second AI winter (1980s - 1990s): Early AI research, including the Perceptron, faced limitations that led to disillusionment and reduced funding in the AI field, resulting in the second AI winter.
TCP/IP is developed in order to build the ARPANET (1983)

Sir Tim Berners-Lee introduces the HyperText Transfer Protocol (1989): The development of TCP/IP provided the necessary networking protocols for the ARPANET, which eventually evolved into the modern Internet. This infrastructure enabled Tim Berners-Lee to create the World Wide Web, including the HTTP protocol for web communication.
The American "moon landing" (1969)

Although not directly linked to the other events in terms of causal relationships, the moon landing represented a significant technological achievement that influenced subsequent innovation in computer science and engineering.
The second AI winter (1980s - 1990s)

Deep neural networks (2000s - present): After the second AI winter, advances in computational power, algorithms, and data availability led to the resurgence of interest in AI, particularly in deep neural networks, which have achieved significant breakthroughs in various fields.
Causal Relationships
The mathematical foundations laid by ancient civilizations influenced later mathematical and scientific discoveries.
Edison's work on the lightbulb facilitated the development of electronic devices and computers.
The creation of the ENIAC enabled early AI research, including the Perceptron.
The limitations of early AI models contributed to the second AI winter.
The development of TCP/IP protocols laid the groundwork for the Internet and the World Wide Web.
The technological progress exemplified by the moon landing inspired further advancements in computing and engineering.
The challenges of the AI winter led to improved methods and technologies, culminating in the success of deep neural networks.

7. Let's take a look at each decade from the 1950s to the present day, identifying the greatest computer science technological achievement of each decade and the most popular programming language.

1950s
Greatest Technological Achievement: The creation of FORTRAN (1957)

Justification: FORTRAN (FORmula TRANslation) was the first high-level programming language, designed by IBM. It revolutionized programming by allowing scientists and engineers to write code more efficiently and less error-prone compared to assembly language or machine code.
Most Popular Programming Language: FORTRAN
Reason: As one of the first high-level programming languages, FORTRAN became widely adopted for scientific and engineering applications.
1960s
Greatest Technological Achievement: The development of the ARPANET (1969)

Justification: ARPANET, the precursor to the modern Internet, was established as a network for connecting research institutions and government agencies. It introduced packet switching and laid the groundwork for global networking.
Most Popular Programming Language: COBOL
Reason: COBOL (Common Business-Oriented Language) was widely used in business, finance, and administrative systems for companies and governments.
1970s
Greatest Technological Achievement: The creation of UNIX (1969-1970)

Justification: UNIX, developed at Bell Labs by Ken Thompson, Dennis Ritchie, and others, became the foundation for many operating systems. Its design principles and the development of the C programming language had a lasting impact on software development.
Most Popular Programming Language: C
Reason: C, developed alongside UNIX, became extremely popular due to its efficiency, portability, and influence on many later languages.
1980s
Greatest Technological Achievement: The development of the IBM PC (1981)

Justification: The IBM PC standardized personal computing, leading to widespread adoption in homes and businesses. It set the stage for the personal computer revolution and the proliferation of compatible software and hardware.
Most Popular Programming Language: C
Reason: C continued to dominate due to its versatility and use in operating systems, embedded systems, and application development.
1990s
Greatest Technological Achievement: The emergence of the World Wide Web (1991)

Justification: Sir Tim Berners-Lee introduced the World Wide Web, revolutionizing information sharing and access. It became the backbone of the modern Internet, enabling the proliferation of websites and online services.
Most Popular Programming Language: Java
Reason: Java, introduced by Sun Microsystems in 1995, became extremely popular due to its platform independence ("write once, run anywhere") and widespread use in web development, enterprise applications, and mobile applications.
2000s
Greatest Technological Achievement: The rise of cloud computing (2006)

Justification: Amazon Web Services (AWS) launched its first cloud services, leading to the growth of cloud computing. It transformed how businesses operate, offering scalable, on-demand resources, and fostering innovation in various fields.
Most Popular Programming Language: Java
Reason: Java maintained its popularity due to its robustness, portability, and extensive use in enterprise environments, Android app development, and web applications.
2010s
Greatest Technological Achievement: The breakthrough in deep learning and AI (2012 onwards)

Justification: Deep learning, particularly with the success of AlexNet in 2012, brought significant advances in artificial intelligence, enabling applications in image and speech recognition, autonomous vehicles, and more.
Most Popular Programming Language: Python
Reason: Python became the language of choice for AI and machine learning due to its simplicity, extensive libraries (such as TensorFlow and PyTorch), and active community support.
2020s (up to present)
Greatest Technological Achievement: The advancement of quantum computing (ongoing)

Justification: Quantum computing has made significant strides, with companies like IBM, Google, and others achieving important milestones. While still in its early stages, it promises to revolutionize fields requiring massive computational power.
Most Popular Programming Language: Python
Reason: Python continues to dominate, especially in the fields of AI, data science, and emerging technologies like quantum computing, due to its ease of use and powerful libraries.
Summary
1950s: FORTRAN (Greatest Achievement: Creation of FORTRAN)
1960s: COBOL (Greatest Achievement: Development of ARPANET)
1970s: C (Greatest Achievement: Creation of UNIX)
1980s: C (Greatest Achievement: Development of the IBM PC)
1990s: Java (Greatest Achievement: Emergence of the World Wide Web)
2000s: Java (Greatest Achievement: Rise of cloud computing)
2010s: Python (Greatest Achievement: Breakthrough in deep learning and AI)
2020s: Python (Greatest Achievement: Advancement of quantum computing)
This timeline highlights the key technological achievements and the most popular programming languages of each decade, showing the evolution of the computer science field over time.

8. The war efforts during World War II and the Cold War significantly accelerated the development of computer science innovations. Here are three relevant examples illustrating how these periods led to pivotal advancements in the field:

World War II: The Development of the Colossus Computer
Innovation: Colossus Computer (1943-1944)

Context: The Colossus computer was developed by British engineer Tommy Flowers at Bletchley Park to break the Lorenz cipher used by the German military.
Details: Colossus was the world's first programmable digital electronic computer. It used vacuum tubes to perform boolean and counting operations. The main purpose of Colossus was to decipher the encrypted messages sent by the Germans, significantly aiding the Allied war effort.
Impact: The success of Colossus in codebreaking demonstrated the potential of electronic computing for complex problem-solving, laying the groundwork for post-war developments in computer science and the creation of more advanced computers.
Cold War: The Development of the SAGE System
Innovation: SAGE (Semi-Automatic Ground Environment) System (1950s-1960s)

Context: During the Cold War, the United States developed the SAGE system to detect and respond to potential Soviet bomber attacks.
Details: SAGE was a large-scale, real-time computer system that integrated radar tracking and data processing to monitor and intercept enemy aircraft. It used hundreds of vacuum tubes and was one of the earliest examples of interactive computing.
Impact: The SAGE system introduced several important computer science innovations, including real-time processing, graphical displays (precursors to modern GUIs), and networked communications. These technologies influenced the development of later systems, including air traffic control and early computer networks.
Cold War: The Development of the ARPANET
Innovation: ARPANET (Advanced Research Projects Agency Network) (1969)

Context: Developed by the U.S. Department of Defense’s Advanced Research Projects Agency (ARPA) during the Cold War to ensure communication resilience in the face of a potential nuclear attack.
Details: ARPANET was the first network to implement the TCP/IP protocol suite, allowing diverse computer networks to interconnect and communicate. The primary aim was to create a decentralized communication system that could withstand potential disruptions.
Impact: ARPANET laid the foundation for the modern Internet. Its development included crucial innovations in networking, such as packet switching and the TCP/IP protocols, which are still the backbone of Internet communications today.
Summary of Innovations
World War II: Colossus Computer

First programmable digital electronic computer developed to break German encryption.
Demonstrated the potential of electronic computing for complex tasks.
Cold War: SAGE System

Real-time, interactive computer system for monitoring and intercepting enemy aircraft.
Introduced innovations in real-time processing, graphical displays, and networked communications.
Cold War: ARPANET

First network to implement TCP/IP, leading to the development of the Internet.
Ensured resilient communication, influencing modern networking technologies.
These examples highlight how military needs during wartime drove significant advancements in computer science, many of which have had lasting impacts on technology and society.

9. Aristotle's Law of Excluded Middle, Boolean Logic, and Modern Computing Machines
   Aristotle's Law of Excluded Middle

Definition: Aristotle's Law of Excluded Middle states that for any proposition, either that proposition is true, or its negation is true. Formally, for any proposition
𝑃
P,
𝑃
P is either true (T) or false (F). There is no middle ground or third option.
Implication: This principle is foundational in classical logic, establishing a binary framework where every statement must adhere to being true or false, with no other possibilities.
Boolean Logic

Definition: Boolean logic is a mathematical system used for logical reasoning and computation, based on binary values: true (1) and false (0). It was developed by George Boole in the mid-19th century.
Operations: Boolean logic includes operations like AND, OR, and NOT, which correspond to basic logical operations:
AND: True if both operands are true.
OR: True if at least one operand is true.
NOT: Inverts the truth value.
Foundation in Law of Excluded Middle: Boolean logic inherently relies on the Law of Excluded Middle because it operates within a binary framework, where every variable and resulting value is either 0 or 1, true or false.
Electrical Circuits in Modern Computing Machines

Binary System: Modern computers use binary (base-2) numeral systems, directly mapping to Boolean logic. This binary system aligns with Aristotle’s Law of Excluded Middle, as it represents data using two distinct states: 0 and 1.
Logic Gates: Electrical circuits in computers are built using logic gates (AND, OR, NOT, NAND, NOR, XOR, XNOR), which perform Boolean operations on binary inputs to produce binary outputs.
Transistors: These gates are implemented using transistors, which switch between on (1) and off (0) states, effectively embodying the principles of Boolean logic and, by extension, Aristotle’s Law of Excluded Middle.
Quantum Computers and Aristotle's Law
Quantum Computing Principles

Qubits: Unlike classical bits, which are strictly 0 or 1, qubits can exist in a superposition of states. This means a qubit can represent both 0 and 1 simultaneously, thanks to quantum superposition.
Superposition: A quantum state
∣
𝜓
⟩
∣ψ⟩ can be a combination of
∣
0
⟩
∣0⟩ and
∣
1
⟩
∣1⟩, expressed as
∣
𝜓
⟩
=
𝛼
∣
0
⟩

- 𝛽
  ∣
  1
  ⟩
  ∣ψ⟩=α∣0⟩+β∣1⟩ where
  𝛼
  α and
  𝛽
  β are complex numbers, and
  ∣
  𝛼
  ∣
  2
- ∣
  𝛽
  ∣
  2
  =
  1
  ∣α∣
  2
  +∣β∣
  2
  =1.
  Entanglement and Non-Classical Logic

Entanglement: Qubits can be entangled, meaning the state of one qubit is dependent on the state of another, no matter the distance between them. This phenomenon does not have a classical counterpart and defies simple binary logic.
Measurement: When a qubit is measured, it collapses to one of the classical states (0 or 1), but until measurement, it exists in a superposition of states. This feature introduces probabilistic outcomes that are not strictly adherent to the Law of Excluded Middle during computation.
Implications for Aristotle's Law

Departure from Binary Logic: Quantum computers operate on principles that transcend the binary nature of classical logic. The existence of superpositions and entanglement means that quantum states do not strictly adhere to being purely true or false until measured.
Quantum Logic: Quantum logic allows for more complex logical structures, accommodating superpositions and entanglements. This means that intermediate states exist and are fundamental to quantum computation, presenting a departure from the absolute binary true/false dichotomy dictated by the Law of Excluded Middle.
Summary
Relationship in Classical Computing: Aristotle's Law of Excluded Middle underpins Boolean logic, which in turn forms the basis of modern computing machinery through binary systems and electrical circuits.
Quantum Computing: Quantum computers fundamentally differ by allowing qubits to exist in superpositions, thereby not strictly adhering to the binary constraints of classical logic until measurement occurs. This introduces a new paradigm of computation that can handle intermediate and probabilistic states.
Quantum computing's break from classical binary logic represents a profound shift in how computation and logical operations are understood and implemented, offering the potential for solving problems intractable for classical computers.

10. A prominent example that illustrates the cultural impact of computer science through cinema is the film "WarGames" (1983). A specific scene that stands out is when the protagonist, David Lightman (played by Matthew Broderick), hacks into a military supercomputer named WOPR (War Operation Plan Response) using his personal home computer. This scene encapsulates the tension and fascination with computer technology of the early 1980s.

Description of the Scene
In this pivotal scene, David, a high school student and computer enthusiast, inadvertently accesses the WOPR by searching for a backdoor password. He navigates through various systems, thinking he's accessing a game server, but instead, he triggers a simulation of global thermonuclear war. The military, believing the simulation to be a real Soviet attack, nearly initiates a real-world nuclear response.

Era Reflection: 1980s
Technological Context: The early 1980s saw the advent of personal computing and the beginning of widespread public access to computer technology. The film was made during a time when home computers were becoming more affordable and accessible, leading to a burgeoning interest in hacking and the potential for computer misuse.
Cultural Impact: "WarGames" reflects society's simultaneous fascination and fear of computers. On one hand, there was excitement about the potential of new technology, and on the other, there was anxiety about its potential to cause unintended consequences. The film taps into Cold War anxieties, highlighting fears that technology could lead to catastrophic outcomes if misused.
Societal Viewpoint: The movie showcases both the allure and the danger of computer science. It portrays hackers as intelligent, curious individuals who could potentially disrupt major systems, emphasizing the need for security and ethical considerations in the field of computing. It also reflects the growing recognition of computers' power and the need for responsible use and governance.
Popular Programming Language of the 1980s
During the 1980s, the most popular programming language was C.

Reason: C was widely used due to its versatility, efficiency, and portability. It was the language of choice for system programming, including operating systems and complex applications. Additionally, its influence extended to many other languages that followed.
Summary
The scene from "WarGames" encapsulates the cultural and technological milieu of the 1980s, a period marked by the rise of personal computing and heightened awareness of both the potential and risks associated with computer science. The popularity of the C programming language during this decade underpins the era's technological advancements and the development of foundational software that would shape the future of computing.
